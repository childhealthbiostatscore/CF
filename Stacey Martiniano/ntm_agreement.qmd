---
title: "NTM CT Scoring - Initial Agreement"
author: "Tim Vigers"
date: "today"
date-format: long
format:
  html:
    toc: true
    toc-depth: 5
    toc-float: true
    code-fold: true
    self-contained: true
    fig-cap-location: top
    page-layout: full
    theme:
      light: flatly
      dark: darkly
bibliography: /Users/timvigers/Documents/Miscellaneous/zotero.bib
csl: /Users/timvigers/Documents/Miscellaneous/american-medical-association.csl
editor: source
---

```{r}
#| include: false
library(tidyverse)
library(irr)
library(redcapAPI)
library(Hmisc)
library(DT)
library(RColorBrewer)
library(knitr)
library(pheatmap)
library(arsenal)
# Import data
api <- read.table("/Users/timvigers/Documents/Work/Vigers/CF/Stacey Martiniano/NTM CT Scoring/api.txt",
                  header = F
)
rcon <- redcapConnection(
  url = "https://redcap.ucdenver.edu/api/",
  token = api[1, 1]
)
data <- exportRecordsTyped(rcon, labels = F)
data <- data.frame(lapply(data, as.character))
data$scan_id <- as.numeric(data$scan_id)
# Make scorer IDs by initial
data$scorer <- paste0(
  toupper(substr(data$reader_first_name, 1, 1)),
  toupper(substr(data$reader_last_name, 1, 1))
)
data$scorer[data$scorer == "GG"] <- "GS"
# Deidentify readers
old <- c("AF", "DL", "JW")
new <- c("A", "B", "C")
data$scorer[data$scorer %in% old] <- new[match(data$scorer, old, nomatch = 0)]
# Revert duplicate IDs back to original to match with gold standard
training <- read.csv("/Users/timvigers/Documents/Work/Vigers/CF/Stacey Martiniano/NTM CT Scoring/Data_Clean/training.key.csv")
dups <- read.csv("/Users/timvigers/Documents/Work/Vigers/CF/Stacey Martiniano/NTM CT Scoring/Data_Clean/duplicated.key.csv")
training$Training_id <- as.numeric(sub("T00", "", training$Training_id))
# Change duplicates to OG IDs
old <- dups$duplicated_ssid..dates.shifted..17.days.
new <- dups$original_ssid
data$scan_id[data$scan_id %in% old] <- new[na.omit(match(data$scan_id, old))]
# Change training IDs to OG IDs
old <- training$Training_id
new <- training$Cffid.Scoring.Set
data$scan_id[data$scan_id %in% old] <- new[na.omit(match(data$scan_id, old))]
# Convert to long
data = data %>% select(scan_id,scorer,scoresheet_timestamp,
                    bronchiectasis_rul:atelectasis___6) %>%
  arrange(scan_id,scorer,scoresheet_timestamp) %>%
  group_by(scan_id,scorer) %>%
  mutate(scorer_repeat = row_number()) %>%
  select(-scoresheet_timestamp) %>%
  pivot_longer(bronchiectasis_rul:atelectasis___6,
               names_to = c("feature", "location"),
               names_pattern = "(.*)_(.*)$"
  ) %>%
  mutate(
    feature = gsub("__", "", feature),
    feature = str_to_title(gsub("_", " ", feature))
  )
data$location <- factor(data$location,
                      levels = c(
                        "1", "2", "3", "4", "5", "6", "ling", "lll",
                        "lul", "rll", "rml", "rul"
                      ),
                      labels = c(
                        "RUL", "RML", "RLL", "LUL", "Ling", "LLL",
                        "Ling", "LLL", "LUL", "RLL", "RML", "RUL"
                      )
)
# Maybe not quite so long (one column per rater)
data = data %>% 
  pivot_wider(names_from = c(scorer,scorer_repeat), values_from = value,
              names_sep = "_")
```

Kappa interpretation table @mchughInterraterReliabilityKappa2012:

| Value of Kappa | Level of Agreement | % of Data that are Reliable |
| -------------- | ------------------ | --------------------------- |
| 0–.20          | None               | 0–4%                        |
| .21–.39        | Minimal            | 4–15%                       |
| .40–.59        | Weak               | 15–35%                      |
| .60–.79        | Moderate           | 35–63%                      |
| .80–.90        | Strong             | 64–81%                      |
| Above.90       | Almost Perfect     | 82–100%                     |


