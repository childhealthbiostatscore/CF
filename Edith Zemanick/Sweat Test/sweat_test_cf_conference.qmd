---
title: "Using Sweat Na and Cl to Predict CFTR-RD and CRMS"
author: "Tim Vigers"
date: "today"
date-format: long
format: revealjs
embed-resources: true
editor: source
bibliography: Presentation.bib
csl: /Users/timvigers/GitHub/styles/american-medical-association-10th-edition.csl
---

```{r setup}
#| include: false
library(DiagrammeR)
library(readxl)
library(arsenal)
library(tidyverse)
library(factoextra)
library(pROC)
library(knitr)
library(kableExtra)
library(caret)
library(rpart)
library(rpart.plot)
library(gtsummary)
source("/Users/timvigers/GitHub/CF/genotype_class_severity.R")
switch(Sys.info()[["sysname"]],
  Windows = {
    home_dir <- "C:/Users/timvigers/OneDrive-TheUniversityofColoradoDenver/Vigers/CF/Edith Zemanick/Sweat Test"
  },
  Darwin = {
    home_dir <- "/Users/timvigers/Library/CloudStorage/OneDrive-TheUniversityofColoradoDenver/Vigers/CF/Edith Zemanick/Sweat Test"
  }
)
knitr::opts_knit$set(root.dir = home_dir)
```

```{r data cleaning}
# Import exclusion data from Excel
diagnoses <- read_excel("./Data_Cleaned/AM reclassifications All Patients with CF or Non-CF.xlsx")
exclude <- diagnoses$PatientID[!is.na(diagnoses$Exclude) |
  diagnoses$Code == "Exclude"]
exclude <- exclude[!is.na(exclude)]
# Import data
df <- read.csv("./Data_Raw/Copy of Sweat Tests_AllFor202558_revised01Feb2021.csv",
  na.strings = ""
)
n1 <- nrow(df)
n2 <- length(unique(df$PatientID))
# Remove exclusions from Angela
df <- df[!(df$PatientID %in% exclude), ]
# Diagnosis per Angela
df$Diagnosis <- factor(diagnoses$Diagnosis[match(df$PatientID, diagnoses$PatientID)],
  levels = c("CFTR-related disorder", "CRMS", "Cystic Fibrosis", "Non-CF"),
  labels = c("CFTR-RD or CRMS", "CFTR-RD or CRMS", "Cystic Fibrosis", "Non-CF")
)
# Add modulator info
cftr <- read.csv("./Data_Raw/Copy of Report CFTR Modulators.csv")
df$Modulator <- apply(df, 1, function(r) {
  mod <- cftr[cftr$Patient.ID == as.numeric(r["PatientID"]), ]
  if (any(duplicated(mod$Age.at.Start..years.))) {
    mod <- mod[-which(duplicated(mod$Age.at.Start..years.)), ]
  }
  mod <- mod[order(mod$Age.at.Start..years.), ]
  age <- as.numeric(r["Age.at.Test.in.Years"])
  as.character(cut(age, c(-Inf, mod$Age.at.Start..years., Inf),
    labels = c("None", mod$Modulator), right = F
  ))
})
# Numeric variables
num_vars <- c("Amount", "Na", "K", "Cl")
num_vars <- c(paste0(num_vars, "_Left"), paste0(num_vars, "_Right"))
df[, num_vars] <- suppressWarnings(lapply(df[, num_vars], as.numeric))
# Filter per Maxie's code
exclude2 <- c(
  22, 23, 24, 29, 31, 474, 483, 484, 485, 505, 510, 785, 791, 792, 793
)
no_dem <- unique(df$PatientID[df$Demographics.Available != "TRUE" | is.na(df$Demographics.Available) | df$Sex == ""])
df <- df %>%
  mutate(Age.at.Test.in.Days = Age.at.Test.in.Weeks * 7) %>%
  filter(
    Age.at.Test.in.Years < 50 & Age.at.Test.in.Days >= 2,
    !(Diagnosis == "Non-CF" & PatientID %in% exclude2),
    Modulator == "None"
  ) %>%
  distinct(.) %>%
  filter(Sex != "", Demographics.Available == "TRUE")
# Genotype info
## By severity
df$geno1 <- cf_genotype_class_severity(df$Genotypes1)
df$geno2 <- cf_genotype_class_severity(df$Genotypes2)
df$geno1[df$Diagnosis == "Non-CF"] <- "Not CF"
df$geno2[df$Diagnosis == "Non-CF"] <- "Not CF"
df$geno_risk <- paste(df$geno1, df$geno2, sep = "_")
df$geno_risk <- factor(df$geno_risk,
  levels = c(
    "Severe_Severe", "Mild_Severe", "Mild_Unknown", "Severe_Mild", "Mild_Mild",
    "Not CF_Not CF", "Severe_Unknown", "Unknown_Severe", "Unknown_Unknown"
  ),
  labels = c(
    "High", "Low", "Low", "Low", "Low", "Not CF", "Unknown",
    "Unknown", "Unknown"
  )
)
## By function
df$geno1_class <- cf_genotype_class_severity(df$Genotypes1, out = "class")
df$geno1_class[grep("508", df$Genotypes1)] <- "F508del"
df$geno1_class <- replace(
  df$geno1_class,
  df$geno1_class %in% c("I", "II", "III"), "Minimal function"
)
df$geno1_class <- replace(
  df$geno1_class,
  df$geno1_class %in% c("IV", "V"), "Residual function"
)

df$geno2_class <- cf_genotype_class_severity(df$Genotypes2, out = "class")
df$geno2_class[grep("508", df$Genotypes2)] <- "F508del"
df$geno2_class <- replace(
  df$geno2_class,
  df$geno2_class %in% c("I", "II", "III"), "Minimal function"
)
df$geno2_class <- replace(
  df$geno2_class,
  df$geno2_class %in% c("IV", "V"), "Residual function"
)

df$geno_class <- factor(paste0(
  pmin(df$geno1_class, df$geno2_class), "/",
  pmax(df$geno1_class, df$geno2_class)
))
# Which arm?
df$Arm <- apply(df[, c(
  "Cl", "Cl_Left", "Cl_Right", "Na", "Na_Left", "Na_Right", "K", "K_Left",
  "K_Right"
)], 1, function(r) {
  k <- sum(!is.na(r))
  if (k == 6) {
    return("Both")
  } else if (k == 3) {
    return("Arm Not Noted")
  } else {
    return(NA)
  }
})
# Merge left and right values
df$Amount <- apply(
  df[, c("Amount", "Amount_Left", "Amount_Right")], 1,
  function(r) {
    amt <- suppressWarnings(as.numeric(as.character(r)))
    mean(amt, na.rm = T)
  }
)
df$Cl <- apply(df[, c("Cl", "Cl_Left", "Cl_Right")], 1, function(r) {
  cl <- suppressWarnings(as.numeric(as.character(r)))
  mean(cl, na.rm = T)
})
df$Na <- apply(df[, c("Na", "Na_Left", "Na_Right")], 1, function(r) {
  na <- suppressWarnings(as.numeric(as.character(r)))
  mean(na, na.rm = T)
})
df$K <- apply(df[, c("K", "K_Left", "K_Right")], 1, function(r) {
  k <- suppressWarnings(as.numeric(as.character(r)))
  mean(k, na.rm = T)
})
# Sodium to chloride ratio
df$`Na/Cl Ratio` <- df$Na / df$Cl
high_ratio <- unique(which(df$`Na/Cl Ratio` > 2))
high_cl <- unique(which(df$Cl > 160))
high_na <- unique(which(df$Na > 160))
df <- df[-unique(c(high_ratio, high_cl, high_na)), ]
# Sweat chloride group
df$`Sweat Chloride Group` <- cut(df$Cl, c(-Inf, 30, 60, Inf),
  right = F,
  labels = c("< 30", "30-59", ">60")
)
# Sort
df <- df %>% arrange(PatientID, Age.at.Test.in.Weeks)
# Table options (most variables are not normally distributed)
mycontrols <- tableby.control(
  numeric.test = "kwt",
  numeric.stats = c("Nmiss", "median", "q1q3")
)
labels <- list(
  Age.at.Test.in.Years = "Age (Years)",
  geno_risk = "Genotype Severity", geno_class = "Specific Genotype"
)
# First measure for everyone
t1_df <- df %>%
  group_by(PatientID) %>%
  filter(row_number() == 1)
# Order groups
df$Diagnosis <- factor(df$Diagnosis,
  levels = c("Cystic Fibrosis", "CFTR-RD or CRMS", "Non-CF")
)
```

# Background

## Sweat chloride testing

- Sweat chloride testing is the gold standard for diagnosing Cystic Fibrosis (CF).

| Chloride level (mmol/L) | Diagnosis                |
|-------------------------|--------------------------|
| >= 60                   | Cystic Fibrosis          |
| 30 - 59                 | Intermediate diagnosis   |
| < 30                    | Cystic Fibrosis unlikely |

## Intermediate sweat test

- Can be caused by:
  - CF
  - CF Carrier
  - CFTR-Related Metabolic Syndrome (CRMS)
    - Cystic Fibrosis Screen Positive, Inconclusive Diagnosis (CFSPID)
  - CFTR-related disorder (CFTR-RD)
  
## CRMS/CFSPID

- Inconclusive CF diagnosis following newborn screen
- Elevated IRT, intermediate sweat, and less than 2 disease-causing CFTR variants
- No clinical signs of Cystic Fibrosis
- Should be followed by CF provider to monitor for symptoms of CF @bombieriRecommendationsClassificationDiseases2011
  - Close follow-up, repeat sweat testing, and CFTR sequencing in the first months of life 
  - Then annual follow-up until age 6-7

## CFTR-RD

- Cystic Fibrosis Transmembrane Regulator (CFTR)-related disorders
  - "A clinical entity associated with CFTR dysfunction that does not fulfill diagnostic criteria for CF" @bombieriRecommendationsClassificationDiseases2011
  - Has symptoms, but no positive newborn screen, sweat test, and less than 2 disease-causing CFTR variants
- Should be followed closely by PCP with consultation of a CF provider to monitor for symptoms of CF 

## Using sodium and chloride for diagnosis

- Small studies have shown that 
  - The ratio of sweat sodium to chloride (Na:Cl) is significantly different between pwCF and non-CF patients @hallSweatSodiumChloride1990
  - Suggested cutoffs are anywhere from 1.0-1.2 @hallSweatSodiumChloride1990 @treggiariRoleSweatIon2021
- This ratio may be more helpful in diagnosing patients with intermediate sweat chloride values @treggiariRoleSweatIon2021
- *Can we use both sodium and chloride to better predict CFTR-RD/CRMS?*

# Our data

## Flowchart

![](flowchart.png)

## Table 1 {.scrollable}

```{r}
df %>%
  filter(!is.na(Na) & !is.na(Cl)) %>%
  select(Diagnosis, Age.at.Test.in.Weeks, Sex, Na, Cl, K, Method, geno_class) %>%
  tbl_summary(
    by = Diagnosis, missing_text = "Missing",
    label = list(
      Age.at.Test.in.Weeks = "Age (Weeks)",
      geno_class = "Genotype Class"
    )
  ) %>%
  add_overall() %>%
  add_p(test = list(geno_class ~ "fisher.test"), test.args = all_tests("fisher.test") ~ list(simulate.p.value = TRUE)) %>%
  separate_p_footnotes()
```



## Based on visual inspection

```{r warning=FALSE}
# Visual inspection
ggplot(df, aes(x = Diagnosis, y = `Na/Cl Ratio`)) +
  geom_boxplot() +
  geom_hline(yintercept = 1, color = "red") +
  geom_hline(yintercept = 1.25, color = "red") +
  theme_classic()
```

## Confusion matrix

```{r}
df$nacl_ratio_cat <- cut(df$`Na/Cl Ratio`,
  breaks = c(-Inf, 1, 1.25, Inf), right = F
)
t <- table(df$nacl_ratio_cat, df$Diagnosis)
t <- addmargins(t)
# colorize the diagonal elements in table with non-ints
for (i in 1:3) {
  t[i, i] <- cell_spec(t[i, i],
    bold = T, color = "white", background = "darkgreen"
  )
}
kable(t)
```

## Confusion matrix (proportions)

```{r}
t <- round(prop.table(table(df$nacl_ratio_cat, df$Diagnosis), margin = 2) * 100, 1)
# colorize the diagonal elements in table with non-ints
for (i in 1:(ncol(t))) {
  t[i, i] <- cell_spec(t[i, i],
    bold = T, color = "white", background = "darkgreen"
  )
}
kable(t, digits = 3)
```

::: {.smaller}

*Proportions in the table above are out of the column total (i.e. by diagnosis), not the total number of participants.*

:::

# ROC analyses

## Na/Cl Ratio for CF vs. CFTR-RD or CRMS

```{r}
roc_cf <- roc(
  response = df$`Diagnosis`,
  predictor = df$`Na/Cl Ratio`,
  levels = c("CFTR-RD or CRMS", "Cystic Fibrosis"),
  quiet = T
)
coords_cf <- round(coords(roc_cf, "best", best.method = "youden"), 3)
plot(roc_cf, print.auc = TRUE, col = "blue", print.thres = T)
```

## Na/Cl Ratio for Non-CF vs. CFTR-RD or CRMS

```{r}
roc_non_cf <- roc(
  response = df$`Diagnosis`,
  predictor = df$`Na/Cl Ratio`,
  levels = c("Non-CF", "CFTR-RD or CRMS"),
  quiet = T
)
coords_non_cf <- round(coords(roc_non_cf, "best", best.method = "youden"), 3)
plot(roc_non_cf, print.auc = TRUE, col = "red", print.thres = T)
```

## Histograms by group

Vertical blue line indicates the threshold for CFTR-RD or CRMS vs. CF and red indicates the threshold for non-CF vs. CFTR-RD or CRMS. Na/Cl ratio was log-transformed for easier visualization.

```{r warning=FALSE}
ggplot(
  df[!is.na(df$`Diagnosis`), ],
  aes(log(`Na/Cl Ratio`), fill = `Diagnosis`)
) +
  geom_histogram(alpha = 0.3, position = "identity", bins = 50) +
  geom_vline(xintercept = log(coords_cf$threshold), color = "blue") +
  geom_vline(xintercept = log(coords_non_cf$threshold), color = "red") +
  ylab("Count") +
  xlab("ln(Na/Cl Ratio)") +
  theme_bw()
```

# K-means clustering

## Brief overview

- An approach to splitting up a data set into K non-overlapping groups.

- We want the within-cluster variation to be as low as possible.

- There are several metrics for within-cluster variation, but we usually use the squared Euclidean distance between all pairs of points in a given cluster divided by the number of points in the cluster.

## The K-means clustering algorithm

1. Randomly assign each observation to clusters 1 through K. 

2. Iterate until the cluster assignments stop changing:

  a. For each cluster, compute the cluster centroid (a vector of feature means).
  
  b. Re-assign each point to the cluster corresponding with the closest centroid.

## The K-means clustering algorithm

![](kmeans.jpg)

## K-means pros and cons

| Pros                                         | Cons                                                        |
|----------------------------------------------|-------------------------------------------------------------|
| Easy to implement and understand.            | Must choose k manually.                                     |
| Scales well to large data sets.              | Has trouble when clusters are of varying sizes and density.                                                            |
| Easily adapts to new examples.               | Centroids can be dragged by outliers. |
| Generalizes to clusters of different shapes. | Does not scale well with more dimensions.                        |

# K-means clustering by Na and Cl

```{r warning=FALSE}
set.seed(1017)
# K means
km_df <- df %>%
  select(Diagnosis, Na, Cl) %>%
  drop_na()
km <- kmeans(km_df[, c("Na", "Cl")], centers = 3)
km_df$Cluster <- factor(km$cluster,
  levels = 1:3,
  labels = c("High", "Medium", "Low")
)
centers <- data.frame(km$centers)
centers$Cluster <- rownames(km$centers)
centers$Cluster <- factor(centers$Cluster,
  levels = 1:3,
  labels = c("High", "Medium", "Low")
)
```

## Na by Cl plot

```{r}
ggplot(km_df, aes(x = Na, y = Cl, color = Diagnosis)) +
  geom_point(alpha = 0.5) +
  theme_classic()
```

## Na by Cl plot with k-means centers

```{r}
ggplot(km_df, aes(x = Na, y = Cl, color = Cluster)) +
  geom_point(shape = 20, alpha = 0.1) +
  geom_point(data = centers, size = 5) +
  theme_classic()
```

## K-means centers

```{r}
rownames(centers) <- NULL
centers <- centers %>% column_to_rownames("Cluster")
kable(centers, row.names = T, digits = 1)
```

## Confusion matrix

```{r}
t <- table(km_df$Cluster, km_df$Diagnosis)
t <- addmargins(t)
# colorize the diagonal elements in table with non-ints
for (i in 1:3) {
  t[i, i] <- cell_spec(t[i, i],
    bold = T, color = "white", background = "darkgreen"
  )
}
kable(t, row.names = T)
```

## Confusion matrix (proportions)

```{r}
t <- table(km_df$Cluster, km_df$Diagnosis)
t <- round(prop.table(t, margin = 2) * 100, 1)
# colorize the diagonal elements in table with non-ints
for (i in 1:(ncol(t))) {
  t[i, i] <- cell_spec(t[i, i],
    bold = T, color = "white", background = "darkgreen"
  )
}
kable(t, digits = 3)
```

# Classification trees

```{r}
cart_df <- df %>%
  select(Diagnosis, Na, Cl) %>%
  drop_na()
```

## Brief overview

- Essentially, a regression tree is a way of predicting which category an observation will fall into based on a series of data splitting rules. 

- We want to split the predictor space (the set of possible values for the variables we're interested in) into $R_1, R_2, ... R_J$ non-overlapping regions that minimize some measure of model predictive value (RSS, error rate, etc.).

## Brief overview

- Because in most cases it's impossible to look at every single partition of the predictor space, we use a top-down and greedy approach to splitting up the data.

  - This is a fancy way of saying that we use the data split that gives us the best result at a given step, and don't look at future steps.
  
## Potential issues

- This kind of recursive splitting can results in trees that predict extremely well with the training data but are not generalizable. 

- To avoid this we essentially build one large, overfit tree, and then "prune" it down to its best subtree.

  - Won't go into detail here, but the process is called *cost complexity pruning* and also utilizes cross validation to approximate the test error.

## Baseball example

![](hitters.jpg)

## Classification tree pros and cons

| Pros                                      | Cons                                                                |
|-------------------------------------------|---------------------------------------------------------------------|
| Easy to interpret.                        | Prone to overfitting.                                               |
| Robust to outliers (except when overfit). | Unstable unless well- regularized.                                  |
| Non-linear.                               | Greedy algorithm means the tree is not and optimal global solution. |
| Non-parametric.                           | Can become unwieldy if not pruned correctly.                        |

## All 3 categories

```{r}
set.seed(1017)
cart_full <- rpart(Diagnosis ~ Na + Cl,
  data = cart_df, method = "class",
  control = rpart.control(cp = 0)
)
rpart.plot(cart_full, type = 3, extra = 8)
```

## Confusion matrix

```{r}
t <- table(predict(cart_full, type = "class"), cart_df$Diagnosis)
t <- addmargins(t)
# colorize the diagonal elements in table with non-ints
for (i in 1:3) {
  t[i, i] <- cell_spec(t[i, i],
    bold = T, color = "white", background = "darkgreen"
  )
}
kable(t)
```

## Confusion matrix (proportions)

```{r}
t <- table(predict(cart_full, type = "class"), cart_df$Diagnosis)
t <- round(prop.table(t, margin = 2) * 100, 1)
# colorize the diagonal elements in table with non-ints
for (i in 1:(ncol(t))) {
  t[i, i] <- cell_spec(t[i, i],
    bold = T, color = "white", background = "darkgreen"
  )
}
kable(t, digits = 3)
```

## CF vs. CFTR-RD or CRMS

```{r}
set.seed(1017)
cf_crms <- cart_df[cart_df$Diagnosis != "Non-CF", ]
cf_crms$Diagnosis <- droplevels(cf_crms$Diagnosis)
cart_cf_crms <- rpart(Diagnosis ~ Na + Cl, data = cf_crms, method = "class")
rpart.plot(cart_cf_crms, type = 3, extra = 8)
```

## Confusion matrix

```{r}
t <- table(predict(cart_cf_crms, type = "class"), cf_crms$Diagnosis)
t <- addmargins(t)
# colorize the diagonal elements in table with non-ints
for (i in 1:2) {
  t[i, i] <- cell_spec(t[i, i],
    bold = T, color = "white", background = "darkgreen"
  )
}
kable(t)
```

## Confusion matrix (proportions)

```{r}
t <- table(predict(cart_cf_crms, type = "class"), cf_crms$Diagnosis)
t <- round(prop.table(t, margin = 2) * 100, 1)
# colorize the diagonal elements in table with non-ints
for (i in 1:(ncol(t))) {
  t[i, i] <- cell_spec(t[i, i],
    bold = T, color = "white", background = "darkgreen"
  )
}
kable(t, digits = 3)
```

## Non-CF vs. CFTR-RD or CRMS

```{r}
set.seed(1017)
non_cf_crms <- cart_df[cart_df$Diagnosis != "Cystic Fibrosis", ]
non_cf_crms$Diagnosis <- droplevels(non_cf_crms$Diagnosis)
cart_non_cf_crms <- rpart(Diagnosis ~ Na + Cl,
  data = non_cf_crms,
  method = "class"
)
rpart.plot(cart_non_cf_crms, type = 1, extra = 8)
```

## Confusion matrix

```{r}
t <- table(predict(cart_non_cf_crms, type = "class"), non_cf_crms$Diagnosis)
t <- addmargins(t)
# colorize the diagonal elements in table with non-ints
for (i in 1:2) {
  t[i, i] <- cell_spec(t[i, i],
    bold = T, color = "white", background = "darkgreen"
  )
}
kable(t)
```

## Confusion matrix (proportions)

```{r}
t <- table(predict(cart_non_cf_crms, type = "class"), non_cf_crms$Diagnosis)
t <- round(prop.table(t, margin = 2) * 100, 1)
# colorize the diagonal elements in table with non-ints
for (i in 1:(ncol(t))) {
  t[i, i] <- cell_spec(t[i, i],
    bold = T, color = "white", background = "darkgreen"
  )
}
kable(t, digits = 3)
```

# Support vector machines

## Brief overview

- Aims to find a hyperplane that best separates our groups.
  - In a p-dimensional space, a hyperplane is a flat affine subspace of dimension p − 1.
    - When considering 2 dimensions, this is just a line.
    
## Hyperplane classifiers

![](hyperplane.png)

## The maximal margin classifier

![](mmc.png)

## But what if the boundary isn't just a simple line?

![](nlb.png)

## But what if the boundary isn't just a simple line?

- The *support vector machine* (SVM) is an extension of the support vector classifier where the feature space is enlarged using kernels.
  - This is a little complicated, but is analogous to adding polynomial terms to a regression model.
    - In fact, SVMs are closely related to logistic regression.

## But what if the boundary isn't just a simple line?

![](svm.png)

## SVM pros and cons

- One of the best “out of the box” classifiers.
- Works best when there's fairly clear separation between classes.

## A simple SVM with radial kernel

![](plot_zoom.png)

## Confusion matrix

```{r}
library(e1071)
set.seed(1017)
svmfit <- svm(Diagnosis ~ Na + Cl, data = cart_df)
```

```{r}
t <- table(predict(svmfit, type = "class"), cart_df$Diagnosis)
t <- addmargins(t)
# colorize the diagonal elements in table with non-ints
for (i in 1:2) {
  t[i, i] <- cell_spec(t[i, i],
    bold = T, color = "white", background = "darkgreen"
  )
}
kable(t)
```

## Confusion matrix (proportions)

```{r}
t <- table(predict(svmfit, type = "class"), cart_df$Diagnosis)
t <- round(prop.table(t, margin = 2) * 100, 1)
# colorize the diagonal elements in table with non-ints
for (i in 1:(ncol(t))) {
  t[i, i] <- cell_spec(t[i, i],
    bold = T, color = "white", background = "darkgreen"
  )
}
kable(t, digits = 3)
```

## Comparison of methods

| Method                                           | % Correct CF | % Correct Non-CF | % Correct CFTR-RD or CRMS |
|--------------------------------------------------|--------------|------------------|---------------------------|
| K-means                                          | 93.1         | 81               | 78.6                      |
| Classification tree (all)                        | 98.9         | 99.9             | 21.4                      |
| Classification tree (CF vs. CFTR-RD or CRMS)     | 99.2         |                  | 92.9                      |
| Classification tree (Non-CF vs. CFTR-RD or CRMS) | 39.3         | 99.5             |                           |
| SVM                                              | 98.2         | 99.9             | 0                         |

## Future directions

1. Additional work on the prediction approaches to see if we can improve the results in our current data. 

  - Bagging, boosting, and random forests are all extensions of regression trees that minimize overfitting.

2. Scrap SVMs for now.

3. Would it be worth considering additional variables for these models? Sex, age at test, etc.?

4. Validate final prediction models on new data.

# Questions, comments, suggestions?

## References

---
nocite: |
  @jamesIntroductionStatisticalLearning2021, @robinPROCOpensourcePackage2011, @therneau1997introduction, @metcalf_utility_2024
---

::: {#refs}
:::
