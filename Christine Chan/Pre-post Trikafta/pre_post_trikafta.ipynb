{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85fa964c-c265-4b8d-bc08-0346a4893ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "wd = '/Volumes/PEDS/RI Biostatistics Core/Shared/Shared Projects/Vigers/CF/Christine Chan/Prepost Triakfta/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01787282-ae74-4a77-b4a8-543ec7b9d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHCO data\n",
    "chco_dates = pd.read_csv(wd + 'Data_Cleaned/chco_trikafta_dates.csv')\n",
    "# Fix selected dates\n",
    "chco_dates.loc[chco_dates.MRN == 1053930,'Start'] = \"12/1/2019\"\n",
    "chco_dates.loc[chco_dates.MRN == 1695512,'Start'] = \"12/12/2019\"\n",
    "chco_dates.loc[chco_dates.MRN == 815274,'Start'] = \"12/13/2019\"\n",
    "chco_dates.loc[chco_dates.MRN == 1078408,'Start'] = \"3/9/2020\"\n",
    "chco_dates.loc[chco_dates.MRN == 899717,'Start'] = \"2/14/2020\"\n",
    "# CFRD only\n",
    "chco_dates = chco_dates.loc[chco_dates['CFRD yes=1'] == 1]\n",
    "# Remove those without Trikafta date\n",
    "chco_dates.dropna(subset = ['Start'],inplace = True)\n",
    "# Add glycemic data\n",
    "chco_a1c = pd.read_csv(wd + 'Data_Cleaned/chco_a1c_ogtt.csv')\n",
    "chco = pd.merge(chco_dates,chco_a1c,on = ['MRN'],how = 'left')\n",
    "# Add PFTs\n",
    "chco_pfts = pd.read_csv(wd + 'Data_Cleaned/chco_bmi_pft.csv')\n",
    "idx = chco_pfts['MRN'].isin(chco_dates['MRN']) \n",
    "chco_pfts = chco_pfts[idx]\n",
    "chco = pd.merge(chco,chco_pfts,on = ['MRN','Date'],how = 'outer')\n",
    "# Add CGM\n",
    "chco_cgm = pd.read_csv(wd + 'Data_Cleaned/chco_cgm.csv')\n",
    "chco_cgm['CFF ID'] = [int(re.sub('_.*','',s)) for s in chco_cgm['subject_id']]\n",
    "chco_cgm['Date'] = [re.sub(' .*','',s) for s in chco_cgm['date_cgm_placement']]\n",
    "chco = pd.merge(chco,chco_cgm,on = ['CFF ID','Date'],how = 'outer')\n",
    "# Sort\n",
    "chco.sort_values(by = ['MRN','Date'],inplace = True)\n",
    "# Fill missing \n",
    "fill = ['DOB','CFF ID','Start','Sex','Race','Hispanic/Latinx','Genotypes1','Genotypes2','Pancreatic Status','CFRD Diagnosis Date']\n",
    "chco[fill] = chco.groupby('MRN')[fill].ffill()\n",
    "chco[fill] = chco.groupby('MRN')[fill].bfill()\n",
    "# Write\n",
    "chco.to_csv(wd + 'Data_Cleaned/chco_final.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb9ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montana data\n",
    "adult_dem = pd.read_csv(wd + 'Data_Cleaned/montana_demo_adults.csv')\n",
    "peds_dem = pd.read_csv(wd + 'Data_Cleaned/montana_demo_kids.csv')\n",
    "montana = pd.concat([adult_dem,peds_dem])\n",
    "# Add CGM\n",
    "montana_cgm = pd.read_csv(wd + 'Data_Cleaned/montana_cgm.csv')\n",
    "montana_cgm['CFF ID'] = [int(re.sub('_.*','',s)) for s in montana_cgm['subject_id']]\n",
    "montana_cgm['Date'] = [re.sub(' .*','',s) for s in montana_cgm['date_cgm_placement']]\n",
    "montana = pd.merge(montana,montana_cgm,on = ['CFF ID'],how = 'outer')\n",
    "# Add A1c and OGTTs\n",
    "montana_a1c = pd.read_csv(wd + 'Data_Cleaned/montana_a1c_ogtt.csv')\n",
    "montana_a1c.dropna(subset = ['CFRD Status'],inplace = True)\n",
    "montana = pd.merge(montana,montana_a1c,on = ['CFF ID','Date'],how = 'outer')\n",
    "# Add PFTs\n",
    "montana_pfts_adult = pd.read_csv(wd + 'Data_Cleaned/montana_bmi_pft_adults.csv')\n",
    "montana_pfts_peds = pd.read_csv(wd + 'Data_Cleaned/montana_bmi_pft_kids.csv')\n",
    "montana_pfts = pd.concat([montana_pfts_adult,montana_pfts_peds])\n",
    "montana = pd.merge(montana,montana_pfts,on = ['CFF ID','Date'],how = 'outer')\n",
    "# Sort\n",
    "montana.sort_values(by = ['CFF ID','Date'],inplace = True)\n",
    "# Fill missing \n",
    "fill = ['DOB','Start','Gender','Race','Mixed Race components','Is Patient of Hispanic Origin']\n",
    "montana[fill] = montana.groupby('CFF ID')[fill].ffill()\n",
    "montana[fill] = montana.groupby('CFF ID')[fill].bfill()\n",
    "# Write\n",
    "montana.to_csv(wd + 'Data_Cleaned/montana_final.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffa8adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Washington data\n",
    "uw_dates = pd.read_csv(wd + 'Data_Cleaned/uw_trikafta_dates.csv')\n",
    "uw_dates.drop_duplicates(subset='CFF ID',inplace = True)\n",
    "# Glycemic data\n",
    "uw_a1c = pd.read_csv(wd + 'Data_Cleaned/uw_a1c_ogtt.csv')\n",
    "uw = pd.merge(uw_dates,uw_a1c,on = ['CFF ID'],how = 'outer')\n",
    "# BMI\n",
    "uw_bmi = pd.read_csv(wd + 'Data_Cleaned/uw_bmi.csv')\n",
    "uw = pd.merge(uw,uw_bmi,on = ['CFF ID','Date'],how = 'outer')\n",
    "# PFTs\n",
    "uw_pft = pd.read_csv(wd + 'Data_Cleaned/uw_pft.csv')\n",
    "uw = pd.merge(uw,uw_pft,on = ['CFF ID','Date','Patients height'],how = 'outer')\n",
    "# Write\n",
    "uw.to_csv(wd + 'Data_Cleaned/uw_final.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c5e3adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CFF ID</th>\n",
       "      <th>Start</th>\n",
       "      <th>HbA1c</th>\n",
       "      <th>OGTT Fasting</th>\n",
       "      <th>OGTT Two Hour</th>\n",
       "      <th>Date</th>\n",
       "      <th>BMI Percentile</th>\n",
       "      <th>Encounter Age</th>\n",
       "      <th>Patients height</th>\n",
       "      <th>BMI Value</th>\n",
       "      <th>Measure of FEV1</th>\n",
       "      <th>Patients height.1</th>\n",
       "      <th>Measure of FVC</th>\n",
       "      <th>Predicted Value for FVC</th>\n",
       "      <th>Predicted Value for FEV1</th>\n",
       "      <th>Predicted Value for FEF25-75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1440650</td>\n",
       "      <td>12/2/19</td>\n",
       "      <td>11.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5/10/19</td>\n",
       "      <td>14.79</td>\n",
       "      <td>19.6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.67</td>\n",
       "      <td>160.0</td>\n",
       "      <td>2.24</td>\n",
       "      <td>3.66</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1440650</td>\n",
       "      <td>12/2/19</td>\n",
       "      <td>12.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/4/19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>19.57</td>\n",
       "      <td>2.17</td>\n",
       "      <td>160.0</td>\n",
       "      <td>2.69</td>\n",
       "      <td>3.66</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1440650</td>\n",
       "      <td>12/2/19</td>\n",
       "      <td>13.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7/31/20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.9</td>\n",
       "      <td>160.0</td>\n",
       "      <td>20.78</td>\n",
       "      <td>2.43</td>\n",
       "      <td>160.0</td>\n",
       "      <td>3.05</td>\n",
       "      <td>3.66</td>\n",
       "      <td>3.22</td>\n",
       "      <td>3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1440650</td>\n",
       "      <td>12/2/19</td>\n",
       "      <td>12.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3/1/19</td>\n",
       "      <td>9.09</td>\n",
       "      <td>19.4</td>\n",
       "      <td>160.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.96</td>\n",
       "      <td>160.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.66</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1440650</td>\n",
       "      <td>12/2/19</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6/5/20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>1649554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/26/20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>61.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>1649554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8/17/20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>53.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2472</th>\n",
       "      <td>1649596</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/28/20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>53.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>1649596</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/9/20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>1649596</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12/28/20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2475 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CFF ID    Start  HbA1c  OGTT Fasting  OGTT Two Hour      Date  \\\n",
       "0     1440650  12/2/19   11.7           NaN            NaN   5/10/19   \n",
       "1     1440650  12/2/19   12.4           NaN            NaN   10/4/19   \n",
       "2     1440650  12/2/19   13.5           NaN            NaN   7/31/20   \n",
       "3     1440650  12/2/19   12.8           NaN            NaN    3/1/19   \n",
       "4     1440650  12/2/19   14.0           NaN            NaN    6/5/20   \n",
       "...       ...      ...    ...           ...            ...       ...   \n",
       "2470  1649554      NaN    NaN           NaN            NaN  10/26/20   \n",
       "2471  1649554      NaN    NaN           NaN            NaN   8/17/20   \n",
       "2472  1649596      NaN    NaN           NaN            NaN  10/28/20   \n",
       "2473  1649596      NaN    NaN           NaN            NaN   11/9/20   \n",
       "2474  1649596      NaN    NaN           NaN            NaN  12/28/20   \n",
       "\n",
       "      BMI Percentile  Encounter Age  Patients height  BMI Value  \\\n",
       "0              14.79           19.6            160.0        NaN   \n",
       "1                NaN           20.0            160.0      19.57   \n",
       "2                NaN           20.9            160.0      20.78   \n",
       "3               9.09           19.4            160.0        NaN   \n",
       "4                NaN           20.7              NaN        NaN   \n",
       "...              ...            ...              ...        ...   \n",
       "2470             NaN            0.3             61.0        NaN   \n",
       "2471             NaN            0.1             53.3        NaN   \n",
       "2472             NaN            0.1             53.3        NaN   \n",
       "2473             NaN            0.1              NaN        NaN   \n",
       "2474             NaN            0.2             61.0        NaN   \n",
       "\n",
       "      Measure of FEV1   Patients height.1  Measure of FVC  \\\n",
       "0                 1.67              160.0            2.24   \n",
       "1                 2.17              160.0            2.69   \n",
       "2                 2.43              160.0            3.05   \n",
       "3                 1.96              160.0            2.50   \n",
       "4                  NaN                NaN             NaN   \n",
       "...                ...                ...             ...   \n",
       "2470               NaN                NaN             NaN   \n",
       "2471               NaN                NaN             NaN   \n",
       "2472               NaN                NaN             NaN   \n",
       "2473               NaN                NaN             NaN   \n",
       "2474               NaN                NaN             NaN   \n",
       "\n",
       "      Predicted Value for FVC  Predicted Value for FEV1   \\\n",
       "0                        3.66                       3.23   \n",
       "1                        3.66                       3.23   \n",
       "2                        3.66                       3.22   \n",
       "3                        3.66                       3.23   \n",
       "4                         NaN                        NaN   \n",
       "...                       ...                        ...   \n",
       "2470                      NaN                        NaN   \n",
       "2471                      NaN                        NaN   \n",
       "2472                      NaN                        NaN   \n",
       "2473                      NaN                        NaN   \n",
       "2474                      NaN                        NaN   \n",
       "\n",
       "      Predicted Value for FEF25-75   \n",
       "0                              3.70  \n",
       "1                              3.69  \n",
       "2                              3.67  \n",
       "3                              3.71  \n",
       "4                               NaN  \n",
       "...                             ...  \n",
       "2470                            NaN  \n",
       "2471                            NaN  \n",
       "2472                            NaN  \n",
       "2473                            NaN  \n",
       "2474                            NaN  \n",
       "\n",
       "[2475 rows x 16 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb671a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method drop_duplicates in module pandas.core.frame:\n",
      "\n",
      "drop_duplicates(subset: 'Hashable | Sequence[Hashable] | None' = None, keep: \"Literal['first'] | Literal['last'] | Literal[False]\" = 'first', inplace: 'bool' = False, ignore_index: 'bool' = False) -> 'DataFrame | None' method of pandas.core.frame.DataFrame instance\n",
      "    Return DataFrame with duplicate rows removed.\n",
      "    \n",
      "    Considering certain columns is optional. Indexes, including time indexes\n",
      "    are ignored.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    subset : column label or sequence of labels, optional\n",
      "        Only consider certain columns for identifying duplicates, by\n",
      "        default use all of the columns.\n",
      "    keep : {'first', 'last', False}, default 'first'\n",
      "        Determines which duplicates (if any) to keep.\n",
      "        - ``first`` : Drop duplicates except for the first occurrence.\n",
      "        - ``last`` : Drop duplicates except for the last occurrence.\n",
      "        - False : Drop all duplicates.\n",
      "    inplace : bool, default False\n",
      "        Whether to drop duplicates in place or to return a copy.\n",
      "    ignore_index : bool, default False\n",
      "        If True, the resulting axis will be labeled 0, 1, …, n - 1.\n",
      "    \n",
      "        .. versionadded:: 1.0.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or None\n",
      "        DataFrame with duplicates removed or None if ``inplace=True``.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.value_counts: Count unique combinations of columns.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Consider dataset containing ramen rating.\n",
      "    \n",
      "    >>> df = pd.DataFrame({\n",
      "    ...     'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n",
      "    ...     'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n",
      "    ...     'rating': [4, 4, 3.5, 15, 5]\n",
      "    ... })\n",
      "    >>> df\n",
      "        brand style  rating\n",
      "    0  Yum Yum   cup     4.0\n",
      "    1  Yum Yum   cup     4.0\n",
      "    2  Indomie   cup     3.5\n",
      "    3  Indomie  pack    15.0\n",
      "    4  Indomie  pack     5.0\n",
      "    \n",
      "    By default, it removes duplicate rows based on all columns.\n",
      "    \n",
      "    >>> df.drop_duplicates()\n",
      "        brand style  rating\n",
      "    0  Yum Yum   cup     4.0\n",
      "    2  Indomie   cup     3.5\n",
      "    3  Indomie  pack    15.0\n",
      "    4  Indomie  pack     5.0\n",
      "    \n",
      "    To remove duplicates on specific column(s), use ``subset``.\n",
      "    \n",
      "    >>> df.drop_duplicates(subset=['brand'])\n",
      "        brand style  rating\n",
      "    0  Yum Yum   cup     4.0\n",
      "    2  Indomie   cup     3.5\n",
      "    \n",
      "    To remove duplicates and keep last occurrences, use ``keep``.\n",
      "    \n",
      "    >>> df.drop_duplicates(subset=['brand', 'style'], keep='last')\n",
      "        brand style  rating\n",
      "    1  Yum Yum   cup     4.0\n",
      "    2  Indomie   cup     3.5\n",
      "    4  Indomie  pack     5.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(uw_dates.drop_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7b406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
